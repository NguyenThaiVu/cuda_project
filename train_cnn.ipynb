{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 23:02:03.980232: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-29 23:02:03.980249: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-29 23:02:03.980260: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/home/necphy/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:  tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "  except RuntimeError as e:  print(e)\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Reshape the data to add an extra dimension (for grayscale channel)\n",
    "train_images = train_images.reshape(train_images.shape[0], 1, 28, 28)\n",
    "test_images = test_images.reshape(test_images.shape[0], 1, 28, 28)\n",
    "\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "train_labels = to_categorical(train_labels, 10)\n",
    "test_labels = to_categorical(test_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_in, n_out, activation=None):\n",
    "        super(CustomDense, self).__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Initialize weights and bias\n",
    "        self.w = self.add_weight(shape=(self.n_in, self.n_out), initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.n_out,), initializer='random_normal', trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z = tf.matmul(inputs, self.w) + self.b  \n",
    "        if self.activation:\n",
    "            z = self.activation(z)  \n",
    "        return z  \n",
    "    \n",
    "    \n",
    "class CustomFlatten(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(CustomFlatten, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.reshape(inputs, [tf.shape(inputs)[0], -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 26, 26)        288       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 26, 26)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 32, 13, 13)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 11, 11)        18432     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 64, 11, 11)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 64, 5, 5)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " custom_flatten (CustomFlat  (None, None)              0         \n",
      " ten)                                                            \n",
      "                                                                 \n",
      " custom_dense (CustomDense)  (None, 10)                16010     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34730 (135.66 KB)\n",
      "Trainable params: 34730 (135.66 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), input_shape=(1, 28, 28), use_bias=False, data_format='channels_first'))\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2, data_format='channels_first'))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), use_bias=False, data_format='channels_first'))\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2, data_format='channels_first'))\n",
    "\n",
    "model.add(CustomFlatten())\n",
    "\n",
    "model.add(CustomDense(n_in=64*5*5, n_out=10)) \n",
    "model.add(layers.Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 1s 1ms/step - loss: 0.2706 - accuracy: 0.9235 - val_loss: 0.0731 - val_accuracy: 0.9786\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.9772 - val_loss: 0.0614 - val_accuracy: 0.9812\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0556 - accuracy: 0.9827 - val_loss: 0.0448 - val_accuracy: 0.9844\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.0453 - accuracy: 0.9856 - val_loss: 0.0430 - val_accuracy: 0.9857\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 0s 1ms/step - loss: 0.0389 - accuracy: 0.9878 - val_loss: 0.0444 - val_accuracy: 0.9857\n",
      "313/313 - 0s - loss: 0.0444 - accuracy: 0.9857 - 193ms/epoch - 615us/step\n",
      "Test accuracy: 0.9857\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, batch_size=128, epochs=5, validation_data=(test_images, test_labels))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f'Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save trained weight into `txt` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "weight shape: (3, 3, 1, 32)\n",
      "weight shape: (3, 3, 32, 64)\n",
      "weight shape: (1600, 10)\n",
      "weight shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "PATH_WEIGHT_FOLDER = r\"weight\"\n",
    "\n",
    "weights = model.get_weights()\n",
    "print(len(weights))\n",
    "\n",
    "for weight in weights:\n",
    "    print(f\"weight shape: {weight.shape}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights and biases saved to separate .txt files.\n"
     ]
    }
   ],
   "source": [
    "conv_weight = tf.transpose(weights[0], perm=[3, 2, 0, 1]).numpy()\n",
    "np.savetxt(os.path.join(PATH_WEIGHT_FOLDER, 'conv2d_weights.txt'), conv_weight.flatten(), fmt='%f', delimiter=' ')\n",
    "\n",
    "conv_1_weight = tf.transpose(weights[1], perm=[3, 2, 0, 1]).numpy()\n",
    "np.savetxt(os.path.join(PATH_WEIGHT_FOLDER, 'conv2d_1_weights.txt'), conv_1_weight.flatten(), fmt='%f', delimiter=' ')\n",
    "\n",
    "np.savetxt(os.path.join(PATH_WEIGHT_FOLDER, 'dense_weights.txt'), weights[2].flatten(), fmt='%f', delimiter=' ')\n",
    "np.savetxt(os.path.join(PATH_WEIGHT_FOLDER, 'dense_bias.txt'), weights[3], fmt='%f', delimiter=' ')\n",
    "\n",
    "print(\"Weights and biases saved to separate .txt files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f59683ff010>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_path = 'image/digit_gray.jpg'\n",
    "\n",
    "img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "img = np.array(img)\n",
    "print(img.shape)\n",
    "\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probabilities: [[1.4144456e-02 2.7578779e-02 8.3968640e-05 8.0715420e-11 9.1218239e-01\n",
      "  6.4961128e-08 3.1494787e-05 4.5977071e-02 1.6956728e-06 6.7734014e-08]]\n",
      "Predicted digit: 4\n",
      "Predicted digit probability: 0.9121823906898499\n"
     ]
    }
   ],
   "source": [
    "# Normalize the pixel values to [0, 1]\n",
    "img_normalized = img.astype('float32') / 255.0\n",
    "\n",
    "img_input = np.reshape(img_normalized, (1, 1, 28, 28))\n",
    "\n",
    "predicted_proba = model.predict(img_input, verbose=0)\n",
    "\n",
    "# Print the predicted probabilities for each class (digits 0-9)\n",
    "print(\"Predicted probabilities:\", predicted_proba)\n",
    "predicted_digit = np.argmax(predicted_proba)\n",
    "print(f\"Predicted digit: {predicted_digit}\")\n",
    "print(f\"Predicted digit probability: {predicted_proba[0][predicted_digit]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get output of Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'activation'\n",
    "conv2d_layer = model.get_layer(name=layer_name)\n",
    "\n",
    "intermediate_model = tf.keras.models.Model(inputs=model.input, outputs=conv2d_layer.output)\n",
    "\n",
    "# Get the output of the 'conv2d' layer\n",
    "conv2d_output = intermediate_model.predict(img_input)\n",
    "\n",
    "# Display the shape of the output tensor\n",
    "print(\"Shape of 'conv2d' layer output:\", conv2d_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv2d_output.flatten()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get output Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'custom_flatten'\n",
    "flatten_layer = model.get_layer(name=layer_name)\n",
    "\n",
    "intermediate_model = tf.keras.models.Model(inputs=model.input, outputs=flatten_layer.output)\n",
    "\n",
    "flatten_output = intermediate_model.predict(img_input)\n",
    "\n",
    "# Display the shape of the output tensor\n",
    "print(\"Shape of 'flatten_output' layer output:\", flatten_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flatten_output.flatten()[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get output of FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'custom_dense'\n",
    "dense_layer = model.get_layer(name=layer_name)\n",
    "\n",
    "intermediate_model = tf.keras.models.Model(inputs=model.input, outputs=dense_layer.output)\n",
    "\n",
    "# Get the output of the 'conv2d' layer \n",
    "dense_output = intermediate_model.predict(img_input)\n",
    "\n",
    "# Display the shape of the output tensor\n",
    "print(\"Shape of 'dense' layer output:\", dense_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
